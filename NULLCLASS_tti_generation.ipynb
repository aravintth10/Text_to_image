{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch torchvision Pillow transformers datasets requests fsspec git+https://github.com/openai/CLIP.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport clip\nfrom PIL import Image\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset\nimport torch.nn as nn\nimport torch.optim as optim\nfrom huggingface_hub import login\n\n\nHUGGINGFACE_TOKEN = \"give your hf auth\"\nlogin(token=HUGGINGFACE_TOKEN)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"ViT-B/32\", device=device)\ndataset = load_dataset(\"HuggingFaceM4/COCO\", \"2014\", split=\"train\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = dataset.map(transform_dataset, remove_columns=[col for col in dataset.column_names if col != \"image\" and col != \"captions\"], batched=False)\ndataset = dataset.filter(lambda x: x is not None)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collate_fn(batch):\n    images = []\n    text_captions = []\n    for item in batch:\n        if \"image\" in item and item[\"image\"] is not None and \"captions\" in item:\n            image = item[\"image\"]\n            if not isinstance(image, torch.Tensor):\n                try:\n                    image = torch.tensor(image)\n                except (TypeError, ValueError) as e:\n                    print(f\"Error converting image to tensor: {e}\")\n                    continue\n            images.append(image)\n            text_captions.extend(item[\"captions\"])\n\n    if not images:\n        return {}\n\n    try:\n        image_batch = torch.stack(images)\n        text_tokens = clip.tokenize(text_captions).to(device)\n        return {\"image\": image_batch, \"captions\": text_tokens}\n    except TypeError:\n        print(\"Error stacking images. Inspect batch content:\")\n        for image in images:\n            print(f\"Shape: {image.shape}, dtype: {image.dtype}\")\n        return {}\n\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, latent_dim, img_channels, text_embedding_dim):\n        super(Generator, self).__init__()\n        self.text_embedding_dim = text_embedding_dim\n        self.model = nn.Sequential(\n            nn.Linear(latent_dim + text_embedding_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, img_channels * 64 * 64),\n            nn.Tanh(),\n        )\n\n    def forward(self, z, text_embedding):\n        combined_input = torch.cat((z, text_embedding), dim=1)\n        return self.model(combined_input).view(z.size(0), 3, 64, 64)\n\nclass Discriminator(nn.Module):\n    def __init__(self, img_channels):\n        super(Discriminator, self).__init__()\n        self.model = nn.Sequential(\n            nn.Linear(img_channels * 64 * 64, 256),\n            nn.LeakyReLU(0.2),\n            nn.Linear(256, 1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, img):\n        return self.model(img.view(img.size(0), -1))\n\nlatent_dim = 100\nimg_channels = 3\nlr = 0.0002\nepochs = 5\ntext_embedding_dim = 512  \ngenerator = Generator(latent_dim, img_channels, text_embedding_dim).to(device)\ndiscriminator = Discriminator(img_channels).to(device)\noptimizer_g = optim.Adam(generator.parameters(), lr=lr)\noptimizer_d = optim.Adam(discriminator.parameters(), lr=lr)\ncriterion = nn.BCELoss()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for epoch in range(epochs):\n    for batch in dataloader:\n        if not batch:\n            continue\n        real_images = batch[\"image\"].to(device)\n        batch_size = real_images.size(0)\n        optimizer_d.zero_grad()\n        real_labels = torch.ones(batch_size, 1).to(device)\n        fake_labels = torch.zeros(batch_size, 1).to(device)\n\n        real_loss = criterion(discriminator(real_images), real_labels)\n\n        z = torch.randn(batch_size, latent_dim).to(device)\n        fake_images = generator(z, torch.randn(batch_size, text_embedding_dim).to(device)) #Random text embeddings for now.\n        fake_loss = criterion(discriminator(fake_images.detach()), fake_labels)\n\n        d_loss = real_loss + fake_loss\n        d_loss.backward()\n        optimizer_d.step()\n        optimizer_g.zero_grad()\n        z = torch.randn(batch_size, latent_dim).to(device)\n\n        \n        prompts = [\"a cat\", \"a dog\", \"a bird\", \"a car\"] * (batch_size // 4)\n        prompts = prompts[:batch_size]\n        text_tokens = clip.tokenize(prompts).to(device)\n        with torch.no_grad():\n            text_embedding = model.encode_text(text_tokens).float()\n\n        fake_images = generator(z, text_embedding)\n\n        g_loss = criterion(discriminator(fake_images), real_labels)\n        g_loss.backward()\n        optimizer_g.step()\n\n    print(f\"Epoch [{epoch+1}/{epochs}], d_loss: {d_loss.item():.4f}, g_loss: {g_loss.item():.4f}\")\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"z = torch.randn(1, latent_dim).to(device)\nprompt = [\"a beautiful landscape\"]\ntext_tokens = clip.tokenize(prompt).to(device)\nwith torch.no_grad():\n    text_embedding = model.encode_text(text_tokens).float()\ngenerated_image = generator(z, text_embedding)\ngenerated_image = generated_image.squeeze(0).permute(1, 2, 0).cpu().detach().numpy()\ngenerated_image = (generated_image * 0.5 + 0.5) * 255.0\ngenerated_image = Image.fromarray(generated_image.astype('uint8'))\ngenerated_image.show() ","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}